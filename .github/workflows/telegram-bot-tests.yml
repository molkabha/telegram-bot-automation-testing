name: Telegram Bot Automation Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'smoke'
        type: choice
        options:
        - smoke
        - critical
        - api
        - regression
        - all

env:
  PYTHON_VERSION: '3.11'
  TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
  TELEGRAM_BOT_USERNAME: ${{ secrets.TELEGRAM_BOT_USERNAME }}
  TELEGRAM_TEST_CHAT_ID: ${{ secrets.TELEGRAM_TEST_CHAT_ID }}

jobs:
  lint-and-security:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black flake8 mypy bandit safety
        pip install -r requirements.txt
    
    - name: Format check with Black
      run: black --check --diff .
    
    - name: Lint with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Type check with mypy
      run: mypy telegram_bot_framework.py --ignore-missing-imports
    
    - name: Security check with bandit
      run: bandit -r . -f json -o security-report.json
    
    - name: Check dependencies for security vulnerabilities
      run: safety check --json --output safety-report.json
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          security-report.json
          safety-report.json

  api-tests:
    name: API Tests
    runs-on: ubuntu-latest
    needs: lint-and-security
    if: always() && (needs.lint-and-security.result == 'success' || needs.lint-and-security.result == 'skipped')
    
    strategy:
      matrix:
        test-type: [smoke, critical, api]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run API tests
      env:
        CI: true
        HEADLESS: true
      run: |
        python -m pytest \
          -m "${{ matrix.test-type }}" \
          --tb=short \
          --html=reports/api-test-report-${{ matrix.test-type }}.html \
          --self-contained-html \
          --junitxml=reports/api-junit-${{ matrix.test-type }}.xml \
          tests/
    
    - name: Upload test reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: api-test-reports-${{ matrix.test-type }}
        path: |
          reports/
          logs/

  ui-tests:
    name: UI Tests (Headless)
    runs-on: ubuntu-latest
    needs: lint-and-security
    if: always() && (needs.lint-and-security.result == 'success' || needs.lint-and-security.result == 'skipped')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install Chrome
      uses: browser-actions/setup-chrome@latest
    
    - name: Install ChromeDriver
      uses: nanasess/setup-chromedriver@master
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run UI tests (headless)
      env:
        CI: true
        HEADLESS: true
      run: |
        python -m pytest \
          -m "ui" \
          --tb=short \
          --html=reports/ui-test-report.html \
          --self-contained-html \
          --junitxml=reports/ui-junit.xml \
          tests/
    
    - name: Upload UI test reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ui-test-reports
        path: |
          reports/
          screenshots/
          logs/

  docker-tests:
    name: Docker Tests
    runs-on: ubuntu-latest
    needs: [api-tests]
    if: always() && needs.api-tests.result == 'success'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Build Docker image
      run: |
        docker build -t telegram-bot-test-framework .
    
    - name: Run tests in Docker
      env:
        TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
        TELEGRAM_BOT_USERNAME: ${{ secrets.TELEGRAM_BOT_USERNAME }}
        TELEGRAM_TEST_CHAT_ID: ${{ secrets.TELEGRAM_TEST_CHAT_ID }}
      run: |
        docker run --rm \
          -e TELEGRAM_BOT_TOKEN \
          -e TELEGRAM_BOT_USERNAME \
          -e TELEGRAM_TEST_CHAT_ID \
          -e CI=true \
          -e HEADLESS=true \
          -v ${{ github.workspace }}/reports:/app/reports \
          telegram-bot-test-framework \
          python -m pytest -m "smoke or critical" --html=reports/docker-test-report.html
    
    - name: Upload Docker test reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: docker-test-reports
        path: reports/

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [api-tests]
    if: always() && needs.api-tests.result == 'success'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run performance tests
      env:
        CI: true
        HEADLESS: true
      run: |
        python -m pytest \
          -m "slow" \
          --tb=short \
          --html=reports/performance-test-report.html \
          --self-contained-html \
          tests/
    
    - name: Upload performance reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-reports
        path: reports/

  generate-report:
    name: Generate Consolidated Report
    runs-on: ubuntu-latest
    needs: [api-tests, ui-tests, docker-tests, performance-tests]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: all-reports
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install jinja2 matplotlib pandas
    
    - name: Generate consolidated report
      run: |
        python scripts/generate_consolidated_report.py all-reports/
    
    - name: Upload consolidated report
      uses: actions/upload-artifact@v4
      with:
        name: consolidated-test-report
        path: |
          consolidated-report.html
          test-summary.json
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // Read test summary
          let summary = '';
          try {
            const summaryData = JSON.parse(fs.readFileSync('test-summary.json', 'utf8'));
            const { total, passed, failed, errors } = summaryData;
            const successRate = ((passed / total) * 100).toFixed(1);
            
            summary = `## ğŸ¤– Telegram Bot Test Results
            
            | Metric | Value |
            |--------|-------|
            | Total Tests | ${total} |
            | Passed | ${passed} âœ… |
            | Failed | ${failed} âŒ |
            | Errors | ${errors} âš ï¸ |
            | Success Rate | ${successRate}% |
            
            ${successRate >= 90 ? 'ğŸ‰ Great job!' : successRate >= 75 ? 'âš ï¸ Some issues found' : 'ğŸš¨ Multiple failures detected'}
            `;
          } catch (error) {
            summary = 'âŒ Could not generate test summary';
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [generate-report]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
    - name: Notify on success
      if: needs.generate-report.result == 'success'
      run: |
        echo "âœ… All tests passed successfully!"
        # Add Slack/Discord/Email notification here
    
    - name: Notify on failure
      if: needs.generate-report.result == 'failure'
      run: |
        echo "âŒ Some tests failed!"
        # Add failure notification here